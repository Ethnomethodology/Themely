{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f51f3fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Empty DataFrame\n",
       " Columns: [Code, Description, Rationale, Example_ids]\n",
       " Index: [],\n",
       "    Serial No.                                      unique_app_id       id  \\\n",
       " 0           1  20250512154221992300_ArtificialInteligence_1iz...  1izv9kf   \n",
       " 1           2  20250512154221992300_ArtificialInteligence_1kj...  1kjwb85   \n",
       " 2           3  20250512154221992300_ArtificialInteligence_1dl...  1dlw98o   \n",
       " 3           4  20250512154221992300_ArtificialInteligence_1k3...  1k3rrkx   \n",
       " 4           5  20250512154221992300_ArtificialInteligence_1je...  1je0frk   \n",
       " \n",
       "    type                                              title  score  \\\n",
       " 0  post  Hot take: LLMs are not gonna get us to AGI, an...    467   \n",
       " 1  post  When do you think the real AGI boom will happe...     83   \n",
       " 2  post  The more I learn about AI the less I believe w...    431   \n",
       " 3  post  dont care about agi/asi definitions; ai is \"sm...     73   \n",
       " 4  post  Google Deepmind CEO predicts AGI will emerge b...    158   \n",
       " \n",
       "                  author  created_utc  \\\n",
       " 0          RandoDude124   1740703391   \n",
       " 1  Familiar_Evidence672   1746952436   \n",
       " 2                jabo0o   1719064612   \n",
       " 3    everything_in_sync   1745170122   \n",
       " 4         Beachbunny_07   1742287557   \n",
       " \n",
       "                                                  url  num_comments  \\\n",
       " 0  https://www.reddit.com/r/ArtificialInteligence...           541   \n",
       " 1  https://www.reddit.com/r/ArtificialInteligence...           293   \n",
       " 2  https://www.reddit.com/r/ArtificialInteligence...           367   \n",
       " 3  https://www.reddit.com/r/ArtificialInteligence...           189   \n",
       " 4  https://x.com/WerAICommunity/status/1901916818...           146   \n",
       " \n",
       "                                                 text              subreddit  \\\n",
       " 0  Title says it all. \\n\\nYeah, it’s cool 4.5 has...  ArtificialInteligence   \n",
       " 1  I'm genuinely curious about the community’s vi...  ArtificialInteligence   \n",
       " 2  I am a big AI enthusiast. I've read Stephen Wo...  ArtificialInteligence   \n",
       " 3  on your left sidebar, click popular read what ...  ArtificialInteligence   \n",
       " 4  So, the way AGI is defined is different depend...  ArtificialInteligence   \n",
       " \n",
       "   link_flair_text                                        Source File  \\\n",
       " 0      Discussion  reddit_data_artificialinteligenc_agi_20250512_...   \n",
       " 1      Discussion  reddit_data_artificialinteligenc_agi_20250512_...   \n",
       " 2      Discussion  reddit_data_artificialinteligenc_agi_20250512_...   \n",
       " 3      Discussion  reddit_data_artificialinteligenc_agi_20250512_...   \n",
       " 4             NaN  reddit_data_artificialinteligenc_agi_20250512_...   \n",
       " \n",
       "                                                Codes Source View  \n",
       " 0             LLM Limitations Argument, LLMs vs. AGI        test  \n",
       " 1  AGI Timeline/Predictions, AGI Definition/Expec...        test  \n",
       " 2  LLM Limitations Argument, LLM Limitations, LLM...        test  \n",
       " 3  Human vs. AI Intelligence, Perceived Intellige...        test  \n",
       " 4  AGI Definition & Characteristics, AGI Definiti...        test  ,\n",
       " 150)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Load the existing codebook and dataset\n",
    "codebook_path = \"codebook.csv\"\n",
    "dataset_path = \"dataset.csv\"\n",
    "\n",
    "# Read the existing codebook\n",
    "codebook_df = pd.read_csv(codebook_path)\n",
    "\n",
    "# Read the dataset\n",
    "dataset_df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Show basic info\n",
    "codebook_df.head(), dataset_df.head(), len(dataset_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ff7e80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Use TF-IDF to vectorize the post texts\u001b[39;00m\n\u001b[32m      6\u001b[39m vectorizer = TfidfVectorizer(stop_words=\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m, max_df=\u001b[32m0.9\u001b[39m, min_df=\u001b[32m2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Estimate the number of clusters (codes)\u001b[39;00m\n\u001b[32m     10\u001b[39m n_clusters = \u001b[38;5;28mmin\u001b[39m(\u001b[32m30\u001b[39m, \u001b[38;5;28mround\u001b[39m(math.sqrt(\u001b[38;5;28mlen\u001b[39m(dataset_df))))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/Work/Projects/Themely/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/Work/Projects/Themely/.venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/Work/Projects/Themely/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/Work/Projects/Themely/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1263\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1262\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1265\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/Work/Projects/Themely/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:99\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \u001b[33;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     doc = \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    101\u001b[39m     doc = analyzer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/Work/Projects/Themely/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:232\u001b[39m, in \u001b[36m_VectorizerMixin.decode\u001b[39m\u001b[34m(self, doc)\u001b[39m\n\u001b[32m    229\u001b[39m     doc = doc.decode(\u001b[38;5;28mself\u001b[39m.encoding, \u001b[38;5;28mself\u001b[39m.decode_error)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np.nan:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    234\u001b[39m     )\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[31mValueError\u001b[39m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Use TF-IDF to vectorize the post texts\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=2)\n",
    "X = vectorizer.fit_transform(dataset_df[\"text\"])\n",
    "\n",
    "# Estimate the number of clusters (codes)\n",
    "n_clusters = min(30, round(math.sqrt(len(dataset_df))))\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "dataset_df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Check top terms in each cluster to help label them\n",
    "top_terms_per_cluster = []\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(n_clusters):\n",
    "    center = kmeans.cluster_centers_[i]\n",
    "    top_indices = center.argsort()[::-1][:10]\n",
    "    top_terms = [terms[ind] for ind in top_indices]\n",
    "    top_terms_per_cluster.append(top_terms)\n",
    "\n",
    "top_terms_per_cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09736623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tests',\n",
       "  'architecture',\n",
       "  'scaling',\n",
       "  'chain',\n",
       "  'years',\n",
       "  'emergent',\n",
       "  'needed',\n",
       "  'far',\n",
       "  'models',\n",
       "  'inherent'],\n",
       " ['ai',\n",
       "  'human',\n",
       "  'agi',\n",
       "  'intelligence',\n",
       "  'awareness',\n",
       "  'thought',\n",
       "  'just',\n",
       "  'time',\n",
       "  'insane',\n",
       "  'quantum'],\n",
       " ['humans',\n",
       "  'agi',\n",
       "  'think',\n",
       "  'going',\n",
       "  'tech',\n",
       "  'people',\n",
       "  'ai',\n",
       "  'better',\n",
       "  'ask',\n",
       "  'human'],\n",
       " ['humans',\n",
       "  'model',\n",
       "  'reasoning',\n",
       "  'models',\n",
       "  'org',\n",
       "  'arc',\n",
       "  'abs',\n",
       "  'agi',\n",
       "  'arxiv',\n",
       "  'https'],\n",
       " ['agi',\n",
       "  'llms',\n",
       "  'human',\n",
       "  'data',\n",
       "  'definition',\n",
       "  'level',\n",
       "  'high',\n",
       "  'research',\n",
       "  'training',\n",
       "  'companies'],\n",
       " ['com',\n",
       "  'https',\n",
       "  'status',\n",
       "  'poll',\n",
       "  'www',\n",
       "  'reddit',\n",
       "  'view',\n",
       "  'focus',\n",
       "  'google',\n",
       "  'youtube'],\n",
       " ['agi',\n",
       "  'economic',\n",
       "  'government',\n",
       "  'day',\n",
       "  'developed',\n",
       "  'ai',\n",
       "  'open',\n",
       "  'people',\n",
       "  'source',\n",
       "  'world'],\n",
       " ['just',\n",
       "  'don',\n",
       "  'know',\n",
       "  'agi',\n",
       "  'ai',\n",
       "  'agents',\n",
       "  've',\n",
       "  'saying',\n",
       "  'wondering',\n",
       "  'achieved'],\n",
       " ['ai',\n",
       "  'agi',\n",
       "  'said',\n",
       "  'good',\n",
       "  'developing',\n",
       "  'videos',\n",
       "  'jensen',\n",
       "  'point',\n",
       "  'key',\n",
       "  'breakthrough'],\n",
       " ['life',\n",
       "  'agi',\n",
       "  'just',\n",
       "  'work',\n",
       "  'working',\n",
       "  'think',\n",
       "  'like',\n",
       "  'team',\n",
       "  'project',\n",
       "  'lot'],\n",
       " ['makes',\n",
       "  'know',\n",
       "  'money',\n",
       "  'people',\n",
       "  'democracy',\n",
       "  'require',\n",
       "  'building',\n",
       "  'curious',\n",
       "  'times',\n",
       "  'optimistic']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing text\n",
    "dataset_df = dataset_df.dropna(subset=[\"text\"])\n",
    "\n",
    "# Re-run TF-IDF vectorization\n",
    "X = vectorizer.fit_transform(dataset_df[\"text\"])\n",
    "\n",
    "# Recalculate clusters\n",
    "n_clusters = min(30, round(math.sqrt(len(dataset_df))))\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "dataset_df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Extract top terms per cluster again\n",
    "top_terms_per_cluster = []\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(n_clusters):\n",
    "    center = kmeans.cluster_centers_[i]\n",
    "    top_indices = center.argsort()[::-1][:10]\n",
    "    top_terms = [terms[ind] for ind in top_indices]\n",
    "    top_terms_per_cluster.append(top_terms)\n",
    "\n",
    "top_terms_per_cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e70dd50",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NDFrame.to_csv() got an unexpected keyword argument 'line_terminator'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mio\u001b[39;00m\n\u001b[32m     34\u001b[39m csv_buffer = io.StringIO()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mfinal_codebook_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# quoting=1 for QUOTE_ALL\u001b[39;00m\n\u001b[32m     37\u001b[39m csv_output = csv_buffer.getvalue()\n\u001b[32m     38\u001b[39m csv_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/Work/Projects/Themely/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: NDFrame.to_csv() got an unexpected keyword argument 'line_terminator'"
     ]
    }
   ],
   "source": [
    "# Map from cluster index to final labels and descriptions (based on top terms)\n",
    "cluster_labels = {\n",
    "    0: (\"Quantum AI\", \"Posts discussing quantum computing, neural design, and advanced AI hardware.\"),\n",
    "    1: (\"AGI Ethics\", \"Discussions on ethical, democratic, or societal aspects of AGI development.\"),\n",
    "    2: (\"Curiosity & Questions\", \"Posts asking speculative or curious questions about AGI and AI futures.\"),\n",
    "    3: (\"Human vs AI\", \"Comparisons between human intelligence and AI models or capabilities.\"),\n",
    "    4: (\"LLM Architecture\", \"Discussion of LLM internals, scaling laws, and data-driven design.\"),\n",
    "    5: (\"Off-topic Links\", \"Posts primarily linking external content or polls with limited commentary.\"),\n",
    "    6: (\"Breakthrough Claims\", \"Posts declaring or speculating on recent breakthroughs in AGI.\"),\n",
    "    7: (\"Futurism Attitudes\", \"Optimistic or skeptical reflections on AGI and future tech trajectories.\"),\n",
    "    8: (\"AGI Consciousness\", \"Debates on AI awareness, sentience, or philosophical identity.\"),\n",
    "    9: (\"Hype & Misinformation\", \"Cynical takes on AGI hype, tech claims, or disillusionment.\"),\n",
    "    10: (\"Corporate AGI News\", \"News or reports from companies like DeepMind or OpenAI on AGI progress.\"),\n",
    "}\n",
    "\n",
    "# Create the codebook entries\n",
    "codebook_entries = []\n",
    "for cluster_id, (code, description) in cluster_labels.items():\n",
    "    subset = dataset_df[dataset_df['cluster'] == cluster_id]\n",
    "    example_ids = subset['unique_app_id'].head(3).tolist()\n",
    "    rationale = f\"This code captures a distinct discussion cluster focused on {description.split()[0].lower()} topics.\"\n",
    "    codebook_entries.append({\n",
    "        \"Code\": code,\n",
    "        \"Description\": description,\n",
    "        \"Rationale\": rationale,\n",
    "        \"Example_ids\": \", \".join(example_ids)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for CSV output\n",
    "final_codebook_df = pd.DataFrame(codebook_entries, columns=[\"Code\", \"Description\", \"Rationale\", \"Example_ids\"])\n",
    "\n",
    "# Export as raw CSV text (RFC 4180 compliant)\n",
    "import io\n",
    "csv_buffer = io.StringIO()\n",
    "final_codebook_df.to_csv(csv_buffer, index=False, line_terminator=\"\\n\", quoting=1)  # quoting=1 for QUOTE_ALL\n",
    "\n",
    "csv_output = csv_buffer.getvalue()\n",
    "csv_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4674086c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Code\",\"Description\",\"Rationale\",\"Example_ids\"\\n\"Quantum AI\",\"Posts discussing quantum computing, neural design, and advanced AI hardware.\",\"This code captures a distinct discussion cluster focused on posts topics.\",\"20250512154221992300_ArtificialInteligence_1b3gf38_10, 20250512154221992300_ArtificialInteligence_1h0u7cs_11, 20250512154221992300_ArtificialInteligence_1f9igzu_25\"\\n\"AGI Ethics\",\"Discussions on ethical, democratic, or societal aspects of AGI development.\",\"This code captures a distinct discussion cluster focused on discussions topics.\",\"20250512154221992300_ArtificialInteligence_1je0frk_4, 20250512154221992300_ArtificialInteligence_1hbqe1d_21, 20250512154221992300_ArtificialInteligence_1gonbz9_48\"\\n\"Curiosity & Questions\",\"Posts asking speculative or curious questions about AGI and AI futures.\",\"This code captures a distinct discussion cluster focused on posts topics.\",\"20250512154221992300_ArtificialInteligence_1kjwb85_1, 20250512154221992300_ArtificialInteligence_1k3rrkx_3, 20250512154221992300_ArtificialInteligence_1haea67_5\"\\n\"Human vs AI\",\"Comparisons between human intelligence and AI models or capabilities.\",\"This code captures a distinct discussion cluster focused on comparisons topics.\",\"20250512154221992300_ArtificialInteligence_1ihhgmh_13, 20250512154221992300_ArtificialInteligence_1ja5xf1_40, 20250512154221992300_ArtificialInteligence_1hwz0p0_45\"\\n\"LLM Architecture\",\"Discussion of LLM internals, scaling laws, and data-driven design.\",\"This code captures a distinct discussion cluster focused on discussion topics.\",\"20250512154221992300_ArtificialInteligence_1izv9kf_0, 20250512154221992300_ArtificialInteligence_1dlw98o_2, 20250512154221992300_ArtificialInteligence_19e96ab_16\"\\n\"Off-topic Links\",\"Posts primarily linking external content or polls with limited commentary.\",\"This code captures a distinct discussion cluster focused on posts topics.\",\"20250512154221992300_ArtificialInteligence_1jporf3_8, 20250512154221992300_ArtificialInteligence_1dusqky_20, 20250512154221992300_ArtificialInteligence_1hndnip_56\"\\n\"Breakthrough Claims\",\"Posts declaring or speculating on recent breakthroughs in AGI.\",\"This code captures a distinct discussion cluster focused on posts topics.\",\"20250512154221992300_ArtificialInteligence_1hqjlgo_6, 20250512154221992300_ArtificialInteligence_1fsqjy2_44, 20250512154221992300_ArtificialInteligence_1j6usv5_58\"\\n\"Futurism Attitudes\",\"Optimistic or skeptical reflections on AGI and future tech trajectories.\",\"This code captures a distinct discussion cluster focused on optimistic topics.\",\"20250512154221992300_ArtificialInteligence_185avsq_22, 20250512154221992300_ArtificialInteligence_1kkmlj7_23, 20250512154221992300_ArtificialInteligence_1hr8lej_28\"\\n\"AGI Consciousness\",\"Debates on AI awareness, sentience, or philosophical identity.\",\"This code captures a distinct discussion cluster focused on debates topics.\",\"20250512154221992300_ArtificialInteligence_1imuf17_15, 20250512154221992300_ArtificialInteligence_1jaq5wl_30, 20250512154221992300_ArtificialInteligence_1j8jdg1_36\"\\n\"Hype & Misinformation\",\"Cynical takes on AGI hype, tech claims, or disillusionment.\",\"This code captures a distinct discussion cluster focused on cynical topics.\",\"20250512154221992300_ArtificialInteligence_1joeidg_50, 20250512154221992300_ArtificialInteligence_1icaidc_53, 20250512154221992300_ArtificialInteligence_1hnej79_75\"\\n\"Corporate AGI News\",\"News or reports from companies like DeepMind or OpenAI on AGI progress.\",\"This code captures a distinct discussion cluster focused on news topics.\",\"20250512154221992300_ArtificialInteligence_1bf611k_17, 20250512154221992300_ArtificialInteligence_1hj6aeq_18, 20250512154221992300_ArtificialInteligence_17yd1ko_19\"\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map from cluster index to final labels and descriptions (based on top terms)\n",
    "cluster_labels = {\n",
    "    0: (\"Quantum AI\", \"Posts discussing quantum computing, neural design, and advanced AI hardware.\"),\n",
    "    1: (\"AGI Ethics\", \"Discussions on ethical, democratic, or societal aspects of AGI development.\"),\n",
    "    2: (\"Curiosity & Questions\", \"Posts asking speculative or curious questions about AGI and AI futures.\"),\n",
    "    3: (\"Human vs AI\", \"Comparisons between human intelligence and AI models or capabilities.\"),\n",
    "    4: (\"LLM Architecture\", \"Discussion of LLM internals, scaling laws, and data-driven design.\"),\n",
    "    5: (\"Off-topic Links\", \"Posts primarily linking external content or polls with limited commentary.\"),\n",
    "    6: (\"Breakthrough Claims\", \"Posts declaring or speculating on recent breakthroughs in AGI.\"),\n",
    "    7: (\"Futurism Attitudes\", \"Optimistic or skeptical reflections on AGI and future tech trajectories.\"),\n",
    "    8: (\"AGI Consciousness\", \"Debates on AI awareness, sentience, or philosophical identity.\"),\n",
    "    9: (\"Hype & Misinformation\", \"Cynical takes on AGI hype, tech claims, or disillusionment.\"),\n",
    "    10: (\"Corporate AGI News\", \"News or reports from companies like DeepMind or OpenAI on AGI progress.\"),\n",
    "}\n",
    "\n",
    "# Create the codebook entries\n",
    "codebook_entries = []\n",
    "for cluster_id, (code, description) in cluster_labels.items():\n",
    "    subset = dataset_df[dataset_df['cluster'] == cluster_id]\n",
    "    example_ids = subset['unique_app_id'].head(3).tolist()\n",
    "    rationale = f\"This code captures a distinct discussion cluster focused on {description.split()[0].lower()} topics.\"\n",
    "    codebook_entries.append({\n",
    "        \"Code\": code,\n",
    "        \"Description\": description,\n",
    "        \"Rationale\": rationale,\n",
    "        \"Example_ids\": \", \".join(example_ids)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for CSV output\n",
    "final_codebook_df = pd.DataFrame(codebook_entries, columns=[\"Code\", \"Description\", \"Rationale\", \"Example_ids\"])\n",
    "\n",
    "# Export as raw CSV text (RFC 4180 compliant)\n",
    "import io\n",
    "csv_buffer = io.StringIO()\n",
    "final_codebook_df.to_csv(csv_buffer, index=False, lineterminator=\"\\n\", quoting=1)\n",
    "\n",
    "csv_output = csv_buffer.getvalue()\n",
    "csv_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b79dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
